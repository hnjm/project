Neural Networks:

- Idee
- Grundsätzliche Funktionsweise
- Layer
- Gewichte
- Backpropagation

CNNs als Spezialfall:

- Besonderheiten
- spezielle Layer
	- Convs
	- Pooling
	- Upsampling
	- Fully Connected
	- ...
- Vor- und Nachteile

Link-Sammlung:

CNNs:
https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/
http://cs231n.github.io/convolutional-networks/
https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
http://deeplearning.net/tutorial/lenet.html
https://deeplearning4j.org/convolutionalnetwork.html

NNs:
http://neuralnetworksanddeeplearning.com/chap1.html
http://neuralnetworksanddeeplearning.com/
http://www.deeplearningbook.org/


1 NEURONALE NETZE:

Im Rahmen des maschinellen Lernens stellen die sogenannten neuronalen Netze einen elementaren Ansatz dar,
der in vielen weiteren Modellen Verwendung findet. Neuronale Netze und auch das maschinelle Lernen an sich
stellen eine andere Herangehensweise dar, als die, die man von "gewöhnlichen" Algorithmen gewohnt ist.

Anstatt dem System eine eindeutige Abfolge von Anweisungen mitzuteilen, um eine konkrete Problemstellung
zu lösen, definiert man ein Modell und konfrontiert dieses mit verschiedenen Beispielen - die Beispiele
sind dabei Tupel aus Eingangsgröße und erwarteter Ausgangsgröße. Die Eingabe sind dabei häufig Bilder und die
Ausgaben konkrete Klassen, um beispielsweise Hunde von Katzen unterscheiden zu können.

Anstatt nun algorithmisch zu definieren, was einen Hund von einer Katze unterscheidet, überlässt man es dem zuvor
erstellten Modell anhand der gegebenen Eingaben und erwarteten Ausgaben, eigenständig Regeln abzuleiten, um mit
dessen Hilfe auch unbekannte Eingaben klassifizieren zu können.

Dieser Ansatz kann auch als "Soft Computing" bezeichnet werden.

1.1 PERZEPTRON

Als elementaren Bestandteil eines neuronalen Netzes kann das sogenannte "Perzeptron" angesehen werden - dieses stellt
die kleinste Einheit eines neuronalen Netztes dar und kann auch als "künstliches Neuron" bezeichnet werden.

Grundsätzlich akzeptiert ein Neuron einen beliebig großen Input bestehend aus den Features x1, x2, ... xn und
berechnet daraus ein Ergebnis.

docu_img_01

Im gezeigten Bild ist beispielsweise ein Neuron dargestellt, das drei Inputgrößen akzeptiert und daraus einen Output
produziert. Um den Output zu berechnen werden sogenannte "Gewichte" (weights) eingeführt. Ob das Neuron 0 oder 1 als
Output liefert, hängt dann davon ab, ob die gewichtete Summe der Eingangsgrößen einen zu definierenden Schwellwert
überschreitet.

Dies kann anhand des nachfolgenden Bilds verdeutlicht werden:

docu_img_02

Dies ist das grundlegende Modell. Grundsätzlich kann man sich das Perzeptron als einen "Entscheidungs-Unterstützer"
vorstellen, der eine Entscheidung trifft, in dem er konkrete Fakten mit einem bestimmten Gewicht versieht.

Das gezeigte Modell ist augenscheinlich sehr simpel und noch sehr weit von dem entfernt, was man als ein neuronales
Netz bezeichnen würde. Es ist allerdings ohne Weiteres denkbar, das gezeigte Model komplexer zu gestalten, indem
mehrere Perzeptrons miteinander verknüpft werden, so dass beispielsweise das nachfolgende Netzwerk entstehen könnte:

docu_img_03

In Grafik <docu_img_02> wurde ein Schwellwert eingeführt, der überschritten werden muss, damit ein Perzeptron
"aktiviert" wird. Um das Modell zu vereinheitlichen, kann der sogenannte "Bias" definiert werden, der den
negativen Schwellwert darstellt. Durch diese Maßnahme kann die Aktivierungsfunktion des Perzeptrons dann geschrieben
werden als:

docu_img_04

Inhaltlich kann das Bias als ein Maß verstanden werden, aus dem hervorgeht, wie "leicht" ein Perzeptron aktiviert
werden kann. Nimmt der Bias einen großen Wert an, so kann das Perzeptron einen Wert von 1 annehmen, auch wenn das
Produkt aus den Gewichten und den Eingangsgrößen einen negativen Wert annimmt. Gleiches gilt selbstverständlich auch für
einen kleinen Bias, der zur Folge hat, dass ein Perzeptron nicht so "leicht" aktiviert werden kann.

1.2 Sigmoid-Neuronen

Eine Weiterentwicklung des zuvor vorgestellten Modells stellen Sigmoid-Neuronen dar. Diese Weiterentwicklung wird
dann erforderlich, wenn das Anpassen der Gewichte - also letztlich das "Lernen" - betrachtet wird. Dabei ist das Ziel,
dass eine "kleine" Anpassung eines Gewichts auch nur eine "kleine" Änderung des Outputs zur Folge hat. Das zuvor
betrachtete Perzeptron ist lediglich in der Lage 0 oder 1 als Output zu liefern, so dass Änderungen an den Gewichten
keine stetige Änderung des Outputs zur Folge haben, sondern "folgenlos" bleiben können bis irgendwann ein Sprung von 0
auf 1 oder umgekehrt stattfindet, was wiederum eine große Änderung darstellt.
Die Weiterentwicklung besteht nun in einer Verfeinerung der Aktivierungsfunktion. Anstatt eine "Sprungfunktion" zu
verwenden, die lediglich 0 und 1 als Funktionswert annehmen kann, wird die sogenannte "Sigmoid Funktion" eingeführt,
die die folgende Form hat:

docu_img_05

Der entscheidende Unterschied kann an den beiden nachfolgenden Grafiken verdeutlicht werden, die jeweils die Kurve
der entsprechenden Funktion darstellen:

docu_img_06

docu_img_07




